{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990ac99e",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Image and text classification using CNN</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3156ff2",
   "metadata": {},
   "source": [
    "Image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea1d06",
   "metadata": {},
   "source": [
    "Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7554d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"imagenet1000_clsidx_to_labels.txt\") as f:\n",
    "    labels = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaefc765",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
    "test_images = [\"cat.png\", \"plane.png\", \"tractor.png\"]\n",
    "\n",
    "transforms = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "for img_path in test_images:\n",
    "    test_image = Image.open(img_path).convert(\"RGB\")\n",
    "    # test_image = T.ToTensor()(test_image)\n",
    "    input_image = transforms(test_image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_image)\n",
    "\n",
    "    predicted_probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "    predicted_probabilities, predicted_classes = torch.topk(predicted_probabilities, 5)\n",
    "\n",
    "    label = labels[predicted_classes[0].item()]\n",
    "    probability = predicted_probabilities[0].item()\n",
    "\n",
    "    plt.imshow(test_image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Predicted class: {label}, Probability: {probability:.4f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b542f4c6",
   "metadata": {},
   "source": [
    "Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c82573cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "288a89a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/hrishikesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/hrishikesh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c73f344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e1816",
   "metadata": {},
   "source": [
    "Creating a CNN for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(embed_dim, 128, 3)\n",
    "        self.conv2 = nn.Conv1d(128, 64, 3)\n",
    "        self.conv3 = nn.Conv1d(64, 32, 3)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.adaptive_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(32 * 24, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.adaptive_pool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb2ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, padding_idx=None):\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        # --- Embedding Layer ---\n",
    "        # We add padding_idx to handle padding correctly\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "\n",
    "        # --- Parallel Convolutions ---\n",
    "        # Your sequential layers are now parallel branches\n",
    "        # We use ModuleList to hold them\n",
    "        filter_sizes = [3, 4, 5] # Kernel sizes for 3-grams, 4-grams, 5-grams\n",
    "        out_channels_list = [128, 64, 32] # Your specified out_channels\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embed_dim,\n",
    "                out_channels=out,\n",
    "                kernel_size=ks\n",
    "            )\n",
    "            for out, ks in zip(out_channels_list, filter_sizes)\n",
    "        ])\n",
    "\n",
    "        # --- Fully Connected Layers ---\n",
    "        # The input size is now the sum of all parallel filter outputs\n",
    "        # 128 + 64 + 32 = 224\n",
    "        total_out_channels = sum(out_channels_list) \n",
    "        \n",
    "        # This re-uses your fc1 hidden size\n",
    "        self.fc1 = nn.Linear(total_out_channels, 128) \n",
    "        \n",
    "        # This re-uses your fc2\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # Re-using your dropout layers\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # NOTE: Removed self.sigmoid\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (batch_size, seq_len)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        # x.shape = (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # Permute for Conv1d: (batch_size, embed_dim, seq_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # --- Apply parallel convolutions and pooling ---\n",
    "        \n",
    "        # Apply each conv, relu, and pool in parallel\n",
    "        pooled_outputs = []\n",
    "        for conv in self.convs:\n",
    "            # 1. Convolve\n",
    "            # conved.shape = (batch_size, out_channels, new_seq_len)\n",
    "            conved = self.relu(conv(x))\n",
    "            \n",
    "            # 2. Global Max Pooling (max-over-time)\n",
    "            # This is the key fix: it finds the max value across the\n",
    "            # entire sequence, resulting in a fixed-size output\n",
    "            # pooled.shape = (batch_size, out_channels, 1)\n",
    "            pooled = F.max_pool1d(conved, kernel_size=conved.shape[2])\n",
    "            \n",
    "            # 3. Squeeze\n",
    "            # squeezed.shape = (batch_size, out_channels)\n",
    "            squeezed = pooled.squeeze(2)\n",
    "            pooled_outputs.append(squeezed)\n",
    "            \n",
    "        # --- Concatenate all parallel outputs ---\n",
    "        # x.shape = (batch_size, 128 + 64 + 32)\n",
    "        x = torch.cat(pooled_outputs, dim=1)\n",
    "\n",
    "        # --- Pass through final linear layers ---\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Return raw logits. This is more stable.\n",
    "        # You should use nn.BCEWithLogitsLoss or nn.CrossEntropyLoss\n",
    "        # in your training loop, as it combines sigmoid/softmax + loss.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "386b5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = nltk.word_tokenize(text)\n",
    "\n",
    "    text = [word for word in text if len(word) > 2]\n",
    "\n",
    "    text = ' '.join([word for word in text if word not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aecbd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewers mentioned watching episode youll...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres family little boy jake thinks...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  one reviewers mentioned watching episode youll...          1\n",
       "1  wonderful little production filming technique ...          1\n",
       "2  thought wonderful way spend time hot summer we...          1\n",
       "3  basically theres family little boy jake thinks...          0\n",
       "4  petter matteis love time money visually stunni...          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"] = df[\"review\"].apply(preprocess_text)\n",
    "df[\"sentiment\"] = df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dcf0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = df[\"review\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eac18154",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<unk>', '<pad>']\n",
    "token_counts = Counter()\n",
    "\n",
    "for tokens in df[\"tokens\"]:\n",
    "    token_counts.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "205b49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_wtoi = OrderedDict()\n",
    "for i, token in enumerate(special_tokens):\n",
    "    vocab_wtoi[token] = i\n",
    "\n",
    "current_index = len(special_tokens)\n",
    "for word, freq in token_counts.most_common():\n",
    "    if not word in vocab_wtoi:\n",
    "        vocab_wtoi[word] = current_index\n",
    "        current_index += 1\n",
    "\n",
    "\n",
    "vocab_itow = {idx: word for word, idx in vocab_wtoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa23487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVocab:\n",
    "    def __init__(self, stoi, itos):\n",
    "        self._stoi = stoi\n",
    "        self._itos = itos\n",
    "        self.unk_index = stoi.get(\"<unk>\", 0) # Default to 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._stoi)\n",
    "\n",
    "    def stoi(self, token):\n",
    "        # Return the index for the token, or the <unk> index\n",
    "        return self._stoi.get(token, self.unk_index)\n",
    "\n",
    "    def itos(self, index):\n",
    "        # Return the token for the index\n",
    "        return self._itos.get(index, \"<unk>\")\n",
    "    \n",
    "    def __call__(self, tokens):\n",
    "        # A helper to convert a list of tokens to indices\n",
    "        return [self.stoi(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5862d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = SimpleVocab(vocab_wtoi, vocab_itow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af4a11ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "glove_vectors = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de6e6ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = glove_vectors.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c69af4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.zeros(vocab_size, embed_dim)\n",
    "\n",
    "for word, idx in vocab_wtoi.items():\n",
    "    if word in glove_vectors:\n",
    "        embedding_matrix[idx] = torch.tensor(glove_vectors[word])\n",
    "    else:\n",
    "        embedding_matrix[idx] = torch.randn(embed_dim) * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8faaa0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([175220, 100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
